---
title: "Lab4"
author: "Annie Adas"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(skimr)
library(tidymodels)
library(caret)
library("corrplot")

```

## Lab 4: Fire and Tree Mortality

The database we'll be working with today includes 36066 observations of individual trees involved in prescribed fires and wildfires occurring over 35 years, from 1981 to 2016. It is a subset of a larger fire and tree mortality database from the US Forest Service (see data description for the full database here: [link](https://www.nature.com/articles/s41597-020-0522-7#Sec10)). Our goal today is to predict the likelihood of tree mortality after a fire.

### Data Exploration

Outcome variable: *yr1status* = tree status (0=alive, 1=dead) assessed one year post-fire.

Predictors: *YrFireName, Species, Genus_species, DBH_cm, CVS_percent, BCHM_m, BTL* (Information on these variables available in the database metadata ([link](https://www.fs.usda.gov/rds/archive/products/RDS-2020-0001-2/_metadata_RDS-2020-0001-2.html))).

```{r message = FALSE}
trees_dat<- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/trees-dat.csv")
```

> Question 1: Recode all the predictors to a zero_based integer form

### Data Splitting
```{r}
df <- trees_dat %>% mutate_if(is.ordered, factor, ordered = FALSE) %>% 
  janitor::clean_names()

```


> Question 2: Create trees_training (70%) and trees_test (30%) splits for the modeling

```{r}
set.seed(123)  # for reproducibility
trees_split <- initial_split(df, prop = .7)
trees_train <- training(trees_split)
trees_test  <- testing(trees_split)
```

>
> Question 3: How many observations are we using for training with this split?

**We are using 25,246 observations for training with this split.**

### Simple Logistic Regression 

Let's start our modeling effort with some simple models: one predictor and one outcome each.

> Question 4: Choose the three predictors that most highly correlate with our outcome variable for further investigation.


```{r}
#create recipe of yr1status on all predictor varialbes
tree_recipe <- recipe(yr1status ~., data = trees_train) %>% 
  #convert stret predictors to binary variables starting at 0
  step_integer(all_string_predictors(), zero_based = T) %>% 
  #prep training data
  prep(trees_train)

#bake recipe with training data
tree_baked_train <- bake(tree_recipe, new_data = trees_train)


```

```{r}
# Obtain correlation matrix
corr_mat <- cor(tree_baked_train)

# Make a correlation plot between the variables
corrplot(corr_mat, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 45, addCoef.col = "black", cl.pos = "n", order = "original")
```


**The three variables that are most highly correlated with our outcome variable is `cvs_percent`, `bchm_m`, and `dbh_cm`.** 



> Question 5: Use glm() to fit three simple logistic regression models, one for each of the predictors you identified.


```{r}
#fit simple logistic regression model with cvs_percent as predictor
model_csv <- glm(yr1status ~ cvs_percent, family = "binomial", data = tree_baked_train)
#fit simple logistic regression model with bchm_m as predictor
model_bchm <- glm(yr1status ~ bchm_m, family = "binomial", data = tree_baked_train)
#fit simple logistic regression model with dbh_cm as predictor
model_dbh <- glm(yr1status ~ dbh_cm, family = "binomial", data = tree_baked_train)

```
 


### Interpret the Coefficients 

We aren't always interested in or able to interpret the model coefficients in a machine learning task. Often predictive accuracy is all we care about.

> Question 6: That said, take a stab at interpreting our model coefficients now.

```{r}
exp(coef(model_csv))
exp(coef(model_bchm))
exp(coef(model_dbh))
```


**model_csv:  The odds of a tree dying one year after a fire increases multiplicatively by 1.08 for each addition 1 percent of the pre fire crown volume that was scoreched or consumed by fire.**

**model_bchm : The odds of a tree dying one year after a fire increases multiplicatively by 1.24 for each addition .01 meteres of maximum bark char from the ground on a tree bole.**

**model_dbh: The odds of a tree dying one year after a fire increases multiplicatively by .94 for each addition centimeter diameter at breast height. **



> Question 7: Now let's visualize the results from these models. Plot the fit to the training data of each model.

```{r message=FALSE}
#plot cvs percent against yr1status with fit to training data
ggplot(trees_train, aes(x = cvs_percent, y = yr1status))+
  geom_point()+
  stat_smooth(method = "glm", se = T, method.args  = list(family = "binomial"))

#plot bchm_m against yr1status with fit to training data
ggplot(trees_train, aes(x = bchm_m, y = yr1status))+
  geom_point()+
  stat_smooth(method = "glm", se = T, method.args  = list(family = "binomial"))

#plot dbh_cm variable against yr1status with fit to training data
ggplot(trees_train, aes(x = dbh_cm, y = yr1status))+
  geom_point()+
  stat_smooth(method = "glm", se = T, method.args  = list(family = "binomial"))
```


### Multiple Logistic Regression

Let's not limit ourselves to a single-predictor model. More predictors might lead to better model performance.

> Question 8: Use glm() to fit a multiple logistic regression called "logistic_full", with all three of the predictors included. Which of these are significant in the resulting model?

### Estimate Model Accuracy

```{r message=FALSE}
logistic_full <- glm(
  yr1status ~ cvs_percent + bchm_m+ dbh_cm,
  family = "binomial", 
  data = trees_train
  )

summary(logistic_full)

```
**`cvs_percent`,`bchm_m`, and `dbh_m` are all significant in the resulting model. **

Now we want to estimate our model's generalizability using resampling.

> Question 9: Use cross validation to assess model accuracy. Use caret::train() to fit four 10-fold cross-validated models (cv_model1, cv_model2, cv_model3, cv_model4) that correspond to each of the four models we've fit so far: three simple logistic regression models corresponding to each of the three key predictors (CVS_percent, DBH_cm, BCHM_m) and a multiple logistic regression model that combines all three predictors.


```{r}
trees_train$yr1status <- factor(trees_train$yr1status)

fitControl <- trainControl(method = "repeatedcv",
                           ## 10-fold CV...
                           number = 10)

cv_model1 <- train(yr1status ~ cvs_percent, data = trees_train, method = "glm", family = "binomial", trControl = fitControl)
cv_model2 <- train(yr1status ~ bchm_m, data = trees_train, method = "glm", family = "binomial", trControl = fitControl)
cv_model3 <-train(yr1status ~ dbh_cm, data = trees_train, method = "glm", family = "binomial", trControl = fitControl)
cv_model4 <-  train(yr1status ~ cvs_percent + bchm_m+ dbh_cm, data = trees_train, method = "glm", family = "binomial", trControl = fitControl)
```

> Question 10: Use caret::resamples() to extract then compare the classification accuracy for each model. (Hint: resamples() wont give you what you need unless you convert the outcome variable to factor form). Which model has the highest accuracy?

Let's move forward with this single most accurate model.

```{r}
resamps <- resamples(list(
  csv = cv_model1,
  bchm = cv_model2,
  dbh = cv_model3,
  full = cv_model4))

summary(resamps)
```
**The model with the highest accuracy is cv_model4, the multiple logistic regression model.**

> Question 11: Compute the confusion matrix and overall fraction of correct predictions by the model.



```{r}
# make predictions on training data
predicted_values <- predict(cv_model4, newdata = trees_train, type = "prob")

# classify anything greater than 0.5 as a 1, and everything else a 0
predicted_class <- ifelse(predicted_values[,2] > 0.5, 1,0) 

# make predicted class and actual values have same factor levels for confusion matrix execution
predicted_class_factor <- factor(predicted_class, levels = c(0, 1))
actual_values_factor <- factor(trees_train$yr1status, levels = c(0, 1))

# create confusion matrix
conf_matrix <- confusionMatrix(predicted_class_factor, actual_values_factor)

conf_matrix

print(paste0("The overall fraction of correct predictions by the model is: ", 100 * round(conf_matrix$overall['Accuracy'],4), "%."))
```



 Question 12: Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

**The confusion matrix is telling me that 1,572 trees in our test set were alive, but the model classified them as dead. It also tells us that 852 trees were dead, but were instead classified as alive.Because there is a class imbalance, trees are going to be identified as dead more often than alive. **

> Question 13: What is the overall accuracy of the model? How is this calculated?

**The overall accuracy of the model is 90.4%. This is calculated by dividing the total number of correctly predicted trees by the total number of trees predicted (22822/25246).**

### Test Final Model

Alright, now we'll take our most accurate model and make predictions on some unseen data (the test data).

> Question 14: Now that we have identified our best model, evaluate it by running a prediction on the test data, trees_test.

```{r}
predicted_values_test <- predict(cv_model4, newdata = trees_test, type = "prob")

# make predictions on test data
predicted_values <- predict(cv_model4, newdata = trees_test, type = "prob")

# classify anything greater than 0.5 as a 1, and everything else a 0
predicted_class <- ifelse(predicted_values[,2] > 0.5, 1,0) 

# make predicted class and actual values have same factor levels for confusion matrix execution
predicted_class_factor <- factor(predicted_class, levels = c(0, 1))
actual_values_factor <- factor(trees_test$yr1status, levels = c(0, 1))

# create confusion matrix
conf_matrix <- confusionMatrix(predicted_class_factor, actual_values_factor)

conf_matrix

print(paste0("The overall fraction of correct predictions by the model is: ", 100 * round(conf_matrix$overall['Accuracy'],4), "%."))
```


> Question 15: How does the accuracy of this final model on the test data compare to its cross validation accuracy? Do you find this to be surprising? Why or why not?
**The accuracy of the final model performs slightly worse than the cross validation accuracy. I don't find this to be surprising because cross validation avoids overfitting and thus improves overall performance.Overall, the models have a very similar performance.** 
